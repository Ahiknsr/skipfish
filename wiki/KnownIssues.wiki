#summary Known issues in skipfish

=Common problems with skipfish=

*Note: [http://code.google.com/p/skipfish/wiki/SkipfishDoc click here] for general project documentation. This document covers some frequently asked issues with _skipfish_ only.*

==Problem #1: Compilation fails==

The most common reason for problems here is that you do not have a properly configured build environment. Please make sure that you have the following components - and their dependencies - installed:

  * [http://gcc.gnu.org/ GNU C Compiler]
  * [http://www.gnu.org/software/make/ GNU Make]
  * [http://www.gnu.org/software/libc/ GNU C Library] (including development headers)
  * [http://www.zlib.net/ zlib] (including development headers)
  * [http://www.openssl.org/ OpenSSL] (including development headers)
  * [http://www.gnu.org/software/libidn/ libidn] (including development headers)

All the development headers must be in your default include path; several vendors mess this up, in which case, you might need to locate the file using `find`, and provide additional `-I/path/to/inclue/files/` options in `CFLAGS`, and `-L/path/to/libraries/` in `LDFLAGS`.

The tool will probably not compile on FreeBSD versions older than 7.0; sorry about that.

==Problem #2: My scan takes forever==

The first thing to check is the number of requests per second displayed by the scanner. It should be at least 200 req/s when scanning remote targets on the Internet, and at least 1000 req/s on local networks. If it is significantly lower, your problem might be just a very slow server; in this case, try changing the `-m` parameter to see if it helps; wait a bit longer; or modify dictionary settings as outlined in `dictionaries/README-FIRST` to reduce the number of requests sent. Note that switching from `complete.wl` to `minimal.wl` may reduce scan time by a factor of 3; using `-Y` may result in a further 20x boost, too.

If the reported scan speed is adequate, but the scan of a relatively small site is still taking hours, you can abort it with `Ctrl-C` and examine the generated report. If you notice recursive, nonsensical nodes in the output, or other significant abnormalities, follow the [http://code.google.com/p/skipfish/wiki/SkipfishDoc#Oy!_Something_went_horribly_wrong! logging build instructions] and contact me to have this fixed. In the meantime, you can use `-r` and `-d` options to set crawl limits, or `-X` to exclude problematic locations.

==Problem #3: I ran it against a "demo" vulnerable application, and...==

Simulated vulnerable applications are usually very simplistic; consider [http://zero.webappsecurity.com/rootlogin.asp.bak this example] from SPI Dynamics: the script responds to certain predefined path inclusion attempts, e.g. `/etc/passwd`, but not to other, more reasonable probes used in our code.

Some journalists and other non-technical users tend to use these vendor-supplied test sites as a benchmark for tested products; therefore, commercial scanners generally try to account for how these demo sites are designed to score well, even if there is no actual security benefit of doing so. 

Given that `skipfish` is an open-source project with no specific commercial incentive, I opted to focus on real-world coverage instead.

==Problem #4: I'm running into display issues==

If your terminal is set to use a non-standard palette or a non-standard color scheme (e.g., black-on-white), `skipfish` output may be harder to read, or may revert your custom settings. If this is a problem, edit `config.h` and comment out the `USE_COLORS` line, then recompile.

Another problem is that if your terminal is set to 80x25, `skipfish` runtime statistics may not fit the screen neatly. If this is the case, please resize to at least 100x35.

==Problem #5: I can't view the report in Safari or Chrome==

This is likely because you browsed to the report via the `file:///` protocol. Certain important security improvements implemented in these two browsers have the unfortunate side effect of crippling local Java<b></b>Script. Please put the report in a local WWW root and navigate to `http://localhost/...` instead; or use Firefox.

==Problem #6: "Abort trap" (Linux, MacOS X) or "Bus error" (FreeBSD)==

This should be fixed in the current version of `skipfish`. Try the newest release, and be sure to issue `make clean all`.

For the curious: the root cause is not a security bug in `skipfish`; instead, the sanity-checking allocator used in the code relies on the ability to use the memory between requested `malloc()` buffer size, and the value returned by `malloc_usable_size()`. Unfortunately, certain security mechanisms enabled by default on some OSes are incompatible with this assumption:

  * The `FORTIFY_SOURCE` feature in GCC is apparently not integrated with `malloc_usable_size()`, and triggers a bogus "buffer overflow" error when this region is accessed, despite being reported as, well, usable.

  * The default allocator settings in FreeBSD - nominally only intended for debugging - clobber this extended region with a 0x5A pattern to detect memory fenceposts when doing `realloc()`.

New versions of `skipfish` disable these features to avoid the problem.

==Problem #7: Scan output directory is huge, what is going on?==

In addition to the list of issues found, the directory contains all the noteworthy HTTP requests and responses seen (including copies of all discovered files), simply to confirm and document scanner findings. While this may result in a rather sizable blob of data, it does not affect the report-viewing experience in any way.

If you do not want to keep this evidence, you can simply delete all the `*.dat` files recursively.

==Problem #8: Memory footprint is huge, what is going on?==

Skipfish should not leak any memory, but it does keep in-memory cache of all crawled documents. The default sample size limit is rather generous - 200 kB; if your site consists of 30,000 large files, you can expect several gigs to be consumed.

There are several simple ways to fix this: use `-I` to narrow down the scan to interesting, dynamic locations on your server; use `-X` to exclude paths with static content tarpits; or use `-s` to limit sample size.

Ultimately, it would probably make sense to store samples for which all analysis is completed on the disk; patches welcome.

==For issues not listed here...==

For any problems not covered on this page, and *particularly* for false positives and false negatives in scan results, please contact me at [mailto:lcamtuf@google.com lcamtuf@google.com].

Have a look at the general [http://code.google.com/p/skipfish/wiki/SkipfishDoc#Oy!_Something_went_horribly_wrong! issue reporting tips] to learn how to collect log data or meaningful crash dumps.