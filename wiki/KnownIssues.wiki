#summary Common problems with skipfish (and how to fix them)

=Common problems with skipfish (and how to fix them)=

*Note: [http://code.google.com/p/skipfish/wiki/SkipfishDoc click here] for general project documentation. This document covers some frequent problems with _skipfish_ only.*

==Problem #1: Compilation fails.==

The most common reason for problems here is that you do not have a properly configured build environment. Please make sure that you have the following components - and their dependencies - installed:

  * [http://gcc.gnu.org/ GNU C Compiler]
  * [http://www.gnu.org/software/make/ GNU Make]
  * [http://www.gnu.org/software/libc/ GNU C Library] (including development headers)
  * [http://www.zlib.net/ zlib] (including development headers)
  * [http://www.openssl.org/ OpenSSL] (including development headers)
  * [http://www.gnu.org/software/libidn/ libidn] (including development headers)

All the development headers must be in your default include path; several vendors mess this up, in which case, you might need to locate the file using `find`, and provide additional `-I/path/to/inclue/files/` options in `CFLAGS`, and `-L/path/to/libraries/` in `LDFLAGS`.

The tool will probably not compile by default on OpenBSD, FreeBSD versions older than 7.0, and other systems without `malloc_usable_size()`; sorry about that.

==Problem #2: My scan is taking forever.==

The first thing to check is the number of requests per second displayed by the scanner. It should be at least 200 req/s when scanning remote targets on the Internet, and at least 1000 req/s on local networks. If it is significantly lower, your problem might be just a very slow server; in this case, try changing the `-m` parameter to see if it helps; wait a bit longer; or modify dictionary settings as outlined in `dictionaries/README-FIRST` to reduce the number of requests sent. Note that switching from `complete.wl` to `minimal.wl` may reduce scan time by a factor of 3; using `-Y` may result in a further 20x boost, too.

If the reported scan speed is adequate, but the scan of a relatively small site is still taking hours, you can abort it with `Ctrl-C` and examine the generated report. If you notice recursive, nonsensical nodes in the output, or other significant abnormalities, <u>please contact me right away to have it fixed</u>. In the meantime, you can use `-r` and `-d` options to set crawl limits, or `-X` to exclude problematic locations. 

Lastly, if everything looks OK, and the site is simply very large, consider using the same `-X` and `-I` options to exclude "tarpit" locations with static content that are not worth brute-forcing. The scanner can't tell interesting places from boring ones, but you almost certainly can make this call.

==Problem #3: I tried it against a "demo" vulnerable application, and...==

Simulated vulnerable applications are usually very simplistic; consider [http://zero.webappsecurity.com/rootlogin.asp.bak this example] from SPI Dynamics: the script responds to certain predefined path inclusion attempts, e.g. `/etc/passwd`, but not to other, more reasonable probes used in our code.

Some journalists and other non-technical users tend to use these vendor-supplied test sites as a benchmark for tested products; therefore, commercial scanners generally try to account for how these demo sites are designed to score well, even if there is no actual security benefit of doing so. 

Given that `skipfish` is an open-source project with no specific commercial incentive, I opted to focus on real-world coverage instead.

==Problem #4: I'm having some display issues.==

If your terminal is set to use a non-standard palette, `skipfish` output may be harder to read. If this is a problem, edit `config.h` and comment out the `USE_COLORS` line, then recompile.

Another problem is that if your terminal is set to 80x25, `skipfish` runtime statistics may not fit the screen neatly. If this is the case, please resize to at least 100x35.

==Problem #5: I can't view the report in Safari or Chrome.==

This is likely because you browsed to the report via the `file:///` protocol. Certain important security improvements implemented in these two browsers have the unfortunate side effect of crippling local Java<b></b>Script. Please put the report in a local WWW root and navigate to `http://localhost/...` instead; or use Firefox.

==Problem #6: "Abort trap" (Linux, MacOS X) or "Bus error" (FreeBSD).==

This should be fixed in the current version of `skipfish`. Try the newest release, and be sure to issue `make clean all`.

For the curious: the root cause is not a security bug in `skipfish`; instead, the sanity-checking allocator used in the code relies on the ability to use the memory between requested `malloc()` buffer size, and the value returned by `malloc_usable_size()`. Unfortunately, certain security mechanisms enabled by default on some OSes are incompatible with this assumption:

  * The `FORTIFY_SOURCE` feature in GCC is apparently not integrated with `malloc_usable_size()`, and triggers a bogus "buffer overflow" error when this region is accessed, despite being reported as, well, usable.

  * The default allocator settings in FreeBSD - nominally only intended for debugging - clobber this extended region with a 0x5A pattern to detect memory fenceposts when doing `realloc()`.

New versions of `skipfish` disable these features to avoid the problem.

==Problem #7: Scan output directory is huge, what is going on?==

In addition to the list of issues found, the directory contains all the noteworthy HTTP requests and responses seen (including copies of all discovered files), simply to confirm and document scanner findings. While this may result in a rather sizable blob of data, it does not affect the report-viewing experience in any way.

If you do not want to keep this evidence, you can simply delete all the `*.dat` files recursively.

==Problem #8: Memory footprint is huge, what is going on?==

Skipfish should not leak any memory, and uses relatively little of it to represent internal data - but it does keep in-memory cache of all crawled documents. The default sample size limit is rather generous: 200 kB. Therefore, if your site consists of 30,000 large videos, you can expect several gigs to be consumed.

There are several simple ways to fix this: use `-I` to narrow down the scan to interesting, dynamic locations on your server; use `-X` to exclude paths with static content tarpits (or skip certain extensions); or use `-s` to limit sample size. If nothing helps, you can also simply just throw some more swap space at the scanner.

Ultimately, it would probably make sense to store samples for which all analysis is completed on the disk; patches welcome.

==Problem #9: The scanner is causing tons of 404 errors!==

Well, it's supposed to - an important part of its functionality is brute-force to discover hidden, accidentally exposed resources on the server; a vast majority of these probes will  yield 404s.

That said, you can use the tool for an orderly crawl with no brute-forcing steps; see `dictionaries/README-FIRST` for advice on how to create and use a null dictionary.

==Problem #10: I am seeing very random false positives and non-existent directories.==

You might have stumbled upon a real bug in _skipfish_ - in this case, see the next section for info on how to report it. That said, but one of the common causes of very erratic scans are web frameworks that start to behave inconsistently under load due to race conditions, backend database limits, and so forth.

Consider this real-world example of a request pattern that led _skipfish_ to assume a shell injection vulnerability:

{{{
* URL http://127.0.0.1/manual/custom-error.html/'`false`' (200, len 9919)
* URL http://127.0.0.1/manual/custom-error.html/'`true`' (200, len 9919)
* URL http://127.0.0.1/manual/custom-error.html/'`uname`' (200, len 8688)
}}}

The two first responses were identical, and the last one differed significantly. The differential probe did what it was supposed to: presence of an underlying vulnerability was the only logical conclusion to make. In reality, the server simply acted up, returning an incomplete page on that last request.

Another example of a similar pattern that caused _skipfish_ to randomly recurse into non-existent directories:

{{{
* URL http://127.0.0.1/manual/custom-error.html/en/ (200, len 3673)
* URL http://127.0.0.1/manual/custom-error.html/en/sfi9876 (200, len 7769)
}}}

To combat this problem, _skipfish_ does some initial server behavior checks - by default, sending 15 consecutive vanilla requests to every fuzzed location, and bailing out early if any of the responses is significantly different than the rest. This short test is not always enough to catch more elusive cases: if you want to make this check more sensitive, you can edit `config.h` and raise the `BH_CHECKS` limit to send more probes, at the expense of prolonging the scan.

The best way to track down the issue is to examine the scan log and request traces, examine problematic responses to see why they differ, and investigate server logs. If you have a difficulty figuring it out, some manual stress-testing may help. 

Lowering the `-m` parameter may reduce the incidence of such glitches, too.

==For issues not listed here...==

For any problems not covered on this page, and *particularly* for:

  * Non-existent files mysteriously appearing in the report,
  * Scans that recursively descent into non-existing paths (`/foo1/foo2/foo1/foo2/...`),
  * Vulnerabilities that do not seem to be present in reality (false positives),
  * Vulnerabilities missed by the scanner (false negatives),
  * Crashes during a scan.

...*please* contact me at [mailto:lcamtuf@google.com lcamtuf@google.com]. Complaining on Twitter will not get the bug fixed :-)

Before filing a report, you may also want to have a look at the general [http://code.google.com/p/skipfish/wiki/SkipfishDoc#Oy!_Something_went_horribly_wrong! issue reporting tips] to learn how to collect log data or meaningful crash dumps.